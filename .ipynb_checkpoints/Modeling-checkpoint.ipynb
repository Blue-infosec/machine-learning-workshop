{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning fo Reaserch\n",
    "\n",
    "#### [Institute of Data Science at Maastricht University](https://www.maastrichtuniversity.nl/research/institute-data-science)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to choose a method based in a research question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1) Get to know your data  \n",
    "- 2) Fit on train data  \n",
    "- 3) Evaluate on test data  \n",
    "- 4) Compare Methods  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/machinelearning.png)  \n",
    "*This image is an extract from [SAP](https://blogs.sap.com/2016/05/11/go-for-the-intelligent-enterprise/) via [@JM_SAP](https://twitter.com/JM_SAP)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any case is easy to identify how to tackle a predictive probles, since is important to identify the method to respond the target variables.\n",
    "    - Pattern: Anomaly Detection\n",
    "    - Cicles: Time Series\n",
    "    - Category: Classification\n",
    "    - Quantity: Regression\n",
    "    - Deep Features: Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import turicreate as tc\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***`turicreate` [documentation](https://apple.github.io/turicreate/docs/api/index.html)***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Airbnb Amsterdam Dataset\n",
    "\n",
    "The Dataset is from [Tom Slee Blog](http://tomslee.net/airbnb-data-collection-get-the-data) and it shows the most recent listings (2017-07-22) from Amsterdam Airbnb Rooms, itself is extracted from [Inside Airbnb](http://insideairbnb.com/) where is possible to make a very nice visual analysis following the hypothesis: Airbnb claims to be part of the \"sharing economy\" and disrupting the hotel industry. However, data shows that the majority of Airbnb listings in most cities are entire homes, many of which are rented all year round - disrupting housing and communities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Copyright Tom Slee, licensed under a [Creative Commons](https://creativecommons.org/licenses/by-nc/2.5/ca/deed.en_US) Attribution-NonCommercial 2.5 Canada License. ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_rooms = pd.read_csv('https://s3.eu-west-3.amazonaws.com/pedrohserrano-datasets/airbnb_amsterdam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rooms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Get to know your data\n",
    "\n",
    "Exploratory data analysis (EDA) is part of any Data Science pipeline, is not Machine Learning itself thought is necessary to develop a method.  \n",
    "EDA is the step where we start delving deeper into the data set beyond the outliers and errors. The knowledge of the dataset might be splitted in two scopes, and asking questions might help: \n",
    "\n",
    "**Intrinsic knowledge**\n",
    "* How is my data distributed?\n",
    "* Are there any correlations in my data?\n",
    "* Are there any confounding factors that explain these correlations?\n",
    "This is the stage where we plot all the data in as many ways as possible. Create many charts, but don't bother making them pretty — these charts are for internal use.\n",
    "\n",
    "**Extrinsic knowledge**\n",
    "* Make sure your data is encoded properly\n",
    "* Make sure your data falls within the expected range, and use domain knowledge whenever possible to define that expected range\n",
    "* Deal with missing data in one way or another: replace it if you can or drop it\n",
    "* Never tidy your data manually because that is not easily reproducible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rooms_cor = df_rooms[['room_type','reviews','overall_satisfaction','accommodates','bedrooms','price']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rooms_cor.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.pairplot(df_rooms_cor, hue='room_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Fit on train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assured that our data is now as clean as we can make it — and armed with some cursory knowledge of the distributions and relationships in our data set — it's time to splitting the data into training and testing sets.\n",
    "\n",
    "A **training set** is a random subset of the data that we use to train our models.\n",
    "\n",
    "A **testing set** is a random subset of the data (mutually exclusive from the training set) that we use to validate our models on unforseen data.\n",
    "\n",
    "Especially in sparse data sets like ours, it's easy for models to **overfit** the data: The model will learn the training set so well that it won't be able to handle most of the cases it's never seen before. This is why it's important for us to build the model with the training set, but score it with the testing set.\n",
    "\n",
    "Note that once we split the data into a training and testing set, we should treat the testing set like it no longer exists: We cannot use any information from the testing set to build our model or else we're cheating.\n",
    "\n",
    "Let's set up our data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into training and testing.\n",
    "We use seed=0 so that everyone running this notebook gets the same results.  In practice, you may set a random seed (or let GraphLab Create pick a random seed for you).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_rooms = tc.SFrame('https://s3.eu-west-3.amazonaws.com/pedrohserrano-datasets/airbnb_amsterdam.csv')\n",
    "train_data, test_data = sf_rooms.random_split(.8,seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's compute the mean of the Prices This is the Target value\n",
    "prices = sf_rooms['price'] # extract the price column of the sales SFrame -- this is now an SArray\n",
    "avg_price = prices.mean() # if you just want the average, the .mean() function\n",
    "print (\"Average price house: %.2f\" % avg_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf_rooms.column_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will learn three (nested) models for predicting room prices. The first model will have the fewest features the second model will add one more feature and the third will add a few more:\n",
    "* Model 1: room_type, neighborhood, reviews\n",
    "* Model 2: Add overall_satisfaction\n",
    "* Model 3: Add accommodates, bedrooms\n",
    "* Model 4: Just numerical, accommodates, bedrooms\n",
    "* Model 5: Add reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_1_features = ['room_type', 'neighborhood', 'reviews']\n",
    "group_2_features = group_1_features + ['overall_satisfaction']\n",
    "group_3_features = group_2_features + ['accommodates', 'bedrooms']\n",
    "group_4_features = ['accommodates', 'bedrooms']\n",
    "group_5_features = group_4_features + ['reviews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the features, learn the weights for the three different models for predicting target = 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_1 = tc.decision_tree_regression.create(train_data, target = 'price', features = group_1_features, validation_set = None)\n",
    "model_2 = tc.random_forest_regression.create(train_data, target = 'price', features = group_2_features, validation_set = None)\n",
    "model_3 = tc.random_forest_regression.create(train_data, target = 'price', features = group_3_features, validation_set = None)\n",
    "model_4 = tc.linear_regression.create(train_data, target = 'price', features = group_4_features, validation_set = None)\n",
    "model_5 = tc.linear_regression.create(train_data, target = 'price', features = group_5_features, validation_set = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the predictions\n",
    "model_1.predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Evaluate on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've learned five models and extracted the model metrics we want to evaluate which model is best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [model_1, model_2, model_3, model_4, model_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_train, evaluation_test = [], []\n",
    "for model in models:\n",
    "    rmse_train = tc.evaluation.rmse(train_data['price'], model.predict(train_data))\n",
    "    rmse_test = tc.evaluation.rmse(test_data['price'], model.predict(test_data))\n",
    "    evaluation_train.append(rmse_train)\n",
    "    evaluation_test.append(rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(models)), evaluation_test, color='r')\n",
    "plt.plot(range(len(models)), evaluation_train, color='g')\n",
    "plt.xticks(range(len(models)), ['Desicion Tree', 'RF + Satisfaction', 'RF All features', 'Linear Bedrooms', 'Linear + Reviews'])\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The closest between metrics of Train and Test the better\n",
    "- The lowest the error the better performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we might pick up the model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the crime you want to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Iterate over the chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = []\n",
    "for repetition in range(1000):\n",
    "    train_data, test_data = sf_rooms.random_split(.8)\n",
    "    model = tc.random_forest_regression.create(train_data, target = 'price', features = group_3_features, validation_set = None, verbose=False)\n",
    "    rmse_test = tc.evaluation.rmse(test_data['price'], model.predict(test_data))\n",
    "    model_scores.append(rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[12, 6])\n",
    "plt.title('Evaluation Score Distribution')\n",
    "sb.distplot(model_scores, color=\"g\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do we want to keep this model? \n",
    "- Shall we construct new features?\n",
    "- What about more data exploration?\n",
    "- Do we still having the same forst question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Documentation of the Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of construct a method is the reproducibility, that is why is important to document the procedures, taking into account versioning control of script for replicable science.\n",
    "\n",
    "Notebooks like this one go a long way toward making our work reproducible. Since we documented every step as we moved along, we have a written record of what we did and why we did it — both in text and code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond recording what we did, we should also document what software and hardware we used to perform our analysis. This typically goes at the top of our notebooks so our readers know what tools to use.\n",
    "\n",
    "[Sebastian Raschka](https://sebastianraschka.com/) created a handy notebook tool for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install watermark\n",
    "#only install once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -a 'Maastricht University' -nmv --packages turicreate,pandas,seaborn,matplotlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
